{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce1aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33a292b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/intrro/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360c28a80ce94e0f88fad8c3703aa4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea02fcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "828bf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp, ob in dataset.items():\n",
    "    if tp == 'train':\n",
    "        df = ob\n",
    "    if tp == 'validation':\n",
    "        val = ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fef7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(range(10))\n",
    "val = val.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7b7c957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mtranslate import translate as TextTranslator\n",
    "\n",
    "geo_data = df.map(lambda example: {'dialogue': TextTranslator(example['dialogue'], 'ka') if example['dialogue'] is not None else '',\n",
    "                                       'summary': TextTranslator(example['summary'], 'ka') if example['summary'] is not None else ''})\n",
    "geo_val_data = val.map(lambda example: {'dialogue': TextTranslator(example['dialogue'], 'ka') if example['dialogue'] is not None else '',\n",
    "                                       'summary': TextTranslator(example['summary'], 'ka') if example['summary'] is not None else ''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13ea7db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(f'bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(f'bert-base-multilingual-cased')\n",
    "def preprocess_data(data_to_process):\n",
    "    #get all the dialogues\n",
    "    inputs = [dialogue for dialogue in data_to_process['dialogue']]\n",
    "    #tokenize the dialogues\n",
    "    model_inputs = tokenizer.encode(inputs, truncation=True, max_length=512)\n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True)\n",
    "    #tokenize the summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        dataset = tokenizer.encode(data_to_process['summary'], truncation=True, max_length=128)\n",
    "        #set labels\n",
    "        model_inputs['labels'] = list(map(int, data_to_process['id']))\n",
    "        #return the tokenized data\n",
    "        #input_ids, attention_mask and labels\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4a2a655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>ამანდა: მე გამომცხვარი ფუნთუშები. გინდა ცოტა?\\...</td>\n",
       "      <td>ამანდამ ფუნთუშები გამოაცხო და ხვალ ჯერის მოუტანს.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>ოლივია: ვის აძლევთ ხმას ამ არჩევნებში?\\r\\nოლივ...</td>\n",
       "      <td>ოლივია და ოლივიე ამ არჩევნებში ლიბერალებს აძლე...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>ტიმი: გამარჯობა, რა ხდება?\\r\\nკიმ: ცუდი განწყო...</td>\n",
       "      <td>კიმმა შეიძლება სცადოს ტიმის მიერ რეკომენდებული...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>ედვარდი: რეიჩელ, მგონი ბელას ვგიჟდები..\\r\\nრეი...</td>\n",
       "      <td>ედვარდი ფიქრობს, რომ ბელაზეა შეყვარებული. რეიჩ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>სემი: ჰეი, მოისმინა რიკმა რაღაცის თქმა\\r\\nსემი...</td>\n",
       "      <td>სემი დაბნეულია, რადგან მოისმინა რიკის წუწუნი მ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13716343</td>\n",
       "      <td>ნევილი: გამარჯობა, ვინმეს ახსოვს რომელ თარიღზე...</td>\n",
       "      <td>ვაიატი ახსენებს ნევილს, რომ მისი ქორწილის წლის...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13611672</td>\n",
       "      <td>ჯონი: გამზ. ხვალინდელი საშინაო დავალება იყო?\\r...</td>\n",
       "      <td>ჯონი არ გამოცხადდა გაკვეთილზე უფროსთან მუშაობი...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13730463</td>\n",
       "      <td>სარა: youtube-ზე ვიპოვე სიმღერა და ვფიქრობ, მო...</td>\n",
       "      <td>სარა ჯეიმსს უგზავნის ინსტრუმენტულ სიმღერას, რო...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13809976</td>\n",
       "      <td>ნოე: როდის და სად ვიკრიბებით? :)\\r\\nმედისონი: ...</td>\n",
       "      <td>ნოეს უნდა შეხვედროდეს, მან სამსახური დატოვა, რ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13809912</td>\n",
       "      <td>მეთი: გინდა პაემანზე წასვლა?\\r\\nაგნესი: ვაი! შ...</td>\n",
       "      <td>მეთი ეპატიჟება აგნესს პაემანზე, რათა უკეთ გაიც...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13818513  ამანდა: მე გამომცხვარი ფუნთუშები. გინდა ცოტა?\\...   \n",
       "1  13728867  ოლივია: ვის აძლევთ ხმას ამ არჩევნებში?\\r\\nოლივ...   \n",
       "2  13681000  ტიმი: გამარჯობა, რა ხდება?\\r\\nკიმ: ცუდი განწყო...   \n",
       "3  13730747  ედვარდი: რეიჩელ, მგონი ბელას ვგიჟდები..\\r\\nრეი...   \n",
       "4  13728094  სემი: ჰეი, მოისმინა რიკმა რაღაცის თქმა\\r\\nსემი...   \n",
       "5  13716343  ნევილი: გამარჯობა, ვინმეს ახსოვს რომელ თარიღზე...   \n",
       "6  13611672  ჯონი: გამზ. ხვალინდელი საშინაო დავალება იყო?\\r...   \n",
       "7  13730463  სარა: youtube-ზე ვიპოვე სიმღერა და ვფიქრობ, მო...   \n",
       "8  13809976  ნოე: როდის და სად ვიკრიბებით? :)\\r\\nმედისონი: ...   \n",
       "9  13809912  მეთი: გინდა პაემანზე წასვლა?\\r\\nაგნესი: ვაი! შ...   \n",
       "\n",
       "                                             summary  \n",
       "0  ამანდამ ფუნთუშები გამოაცხო და ხვალ ჯერის მოუტანს.  \n",
       "1  ოლივია და ოლივიე ამ არჩევნებში ლიბერალებს აძლე...  \n",
       "2  კიმმა შეიძლება სცადოს ტიმის მიერ რეკომენდებული...  \n",
       "3  ედვარდი ფიქრობს, რომ ბელაზეა შეყვარებული. რეიჩ...  \n",
       "4  სემი დაბნეულია, რადგან მოისმინა რიკის წუწუნი მ...  \n",
       "5  ვაიატი ახსენებს ნევილს, რომ მისი ქორწილის წლის...  \n",
       "6  ჯონი არ გამოცხადდა გაკვეთილზე უფროსთან მუშაობი...  \n",
       "7  სარა ჯეიმსს უგზავნის ინსტრუმენტულ სიმღერას, რო...  \n",
       "8  ნოეს უნდა შეხვედროდეს, მან სამსახური დატოვა, რ...  \n",
       "9  მეთი ეპატიჟება აგნესს პაემანზე, რათა უკეთ გაიც...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(geo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be6cb35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/intrro/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-e731ee9574f2e8ee.arrow\n",
      "Loading cached processed dataset at /Users/intrro/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-90aa56b8080ca208.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenize_data = geo_data.map(preprocess_data, batched = True)\n",
    "tokenize_val = geo_val_data.map(preprocess_data, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6c7afff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13817023</td>\n",
       "      <td>A: გამარჯობა ტომ, დაკავებული ხარ ხვალ დღის მეო...</td>\n",
       "      <td>ხვალ ა წავა ცხოველთა თავშესაფარში, რათა შვილის...</td>\n",
       "      <td>[101, 138, 131, 1566, 58859, 97383, 111522, 25...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>13817023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13716628</td>\n",
       "      <td>ემა: მე ახლახან შემიყვარდა ეს მარხვის კალენდარ...</td>\n",
       "      <td>ემას და რობს უყვართ მარხვის კალენდარი. ლორენი ...</td>\n",
       "      <td>[101, 1568, 17566, 131, 68662, 1564, 76564, 25...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>13716628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13829420</td>\n",
       "      <td>ჯეკი: მედისონი ორსულია\\r\\nჯეკი: მაგრამ მას არ ...</td>\n",
       "      <td>მედისონი ორსულადაა, მაგრამ მას არ სურს ამაზე ს...</td>\n",
       "      <td>[101, 1595, 17076, 37183, 131, 68662, 24434, 3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>13829420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13819648</td>\n",
       "      <td>მარლა: &lt;file_photo&gt;\\r\\nმარლა: ნახე რა ვიპოვე ჩ...</td>\n",
       "      <td>მარლამ საწოლის ქვეშ მოკრივეების წყვილი იპოვა.</td>\n",
       "      <td>[101, 1575, 97383, 25559, 131, 133, 23198, 168...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>13819648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728448</td>\n",
       "      <td>რობერტი: მომეცი ამ მუსიკალური მაღაზიის მისამარ...</td>\n",
       "      <td>რობერტს სურს ფრედს გაუგზავნოს მუსიკალური მაღაზ...</td>\n",
       "      <td>[101, 1580, 15812, 54519, 48808, 20320, 131, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>13728448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13817023  A: გამარჯობა ტომ, დაკავებული ხარ ხვალ დღის მეო...   \n",
       "1  13716628  ემა: მე ახლახან შემიყვარდა ეს მარხვის კალენდარ...   \n",
       "2  13829420  ჯეკი: მედისონი ორსულია\\r\\nჯეკი: მაგრამ მას არ ...   \n",
       "3  13819648  მარლა: <file_photo>\\r\\nმარლა: ნახე რა ვიპოვე ჩ...   \n",
       "4  13728448  რობერტი: მომეცი ამ მუსიკალური მაღაზიის მისამარ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ხვალ ა წავა ცხოველთა თავშესაფარში, რათა შვილის...   \n",
       "1  ემას და რობს უყვართ მარხვის კალენდარი. ლორენი ...   \n",
       "2  მედისონი ორსულადაა, მაგრამ მას არ სურს ამაზე ს...   \n",
       "3      მარლამ საწოლის ქვეშ მოკრივეების წყვილი იპოვა.   \n",
       "4  რობერტს სურს ფრედს გაუგზავნოს მუსიკალური მაღაზ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 138, 131, 1566, 58859, 97383, 111522, 25...   \n",
       "1  [101, 1568, 17566, 131, 68662, 1564, 76564, 25...   \n",
       "2  [101, 1595, 17076, 37183, 131, 68662, 24434, 3...   \n",
       "3  [101, 1575, 97383, 25559, 131, 133, 23198, 168...   \n",
       "4  [101, 1580, 15812, 54519, 48808, 20320, 131, 1...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask    labels  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  13817023  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  13716628  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  13829420  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  13819648  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  13728448  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenize_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d35f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a9a8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    'conversation-summ', #save directory\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size= 2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=3,\n",
    "\n",
    "    # fp16=True #available only with CUDA\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=tokenize_data,\n",
    "    eval_dataset=tokenize_val,\n",
    "    #data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_rouge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46c35a77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/intrro/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/trainer.py:2750\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2750\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2753\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/trainer.py:2775\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2774\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2775\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1375\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mshift_tokens_right\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_start_token_id\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1380\u001b[0m     input_ids,\n\u001b[1;32m   1381\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1395\u001b[0m )\n\u001b[1;32m   1397\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/text_summarization/venv-summarization/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:79\u001b[0m, in \u001b[0;36mshift_tokens_right\u001b[0;34m(input_ids, pad_token_id, decoder_start_token_id)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mShift input ids one token to the right.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m shifted_input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mnew_zeros(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 79\u001b[0m shifted_input_ids[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     80\u001b[0m shifted_input_ids[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m decoder_start_token_id\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dbac815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13818513"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(tokenize_data['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a2bd6af",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenize_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, tokenize_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Dataset' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "tokenize_data['labels'] = map(int, tokenize_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395e2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
